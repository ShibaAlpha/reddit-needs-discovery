#!/usr/bin/env python3
"""Redditéœ€æ±‚åˆ†æå™¨"""

import sqlite3
import re
import json
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

DB_PATH = '/Users/openclaw-bot/.openclaw/workspace/reddit-needs-discovery/data/reddit_posts.db'
OUTPUT_DIR = Path('/Users/openclaw-bot/.openclaw/workspace/reddit-needs-discovery/reports')

# ç—›ç‚¹å…³é”®è¯
PAIN_KEYWORDS = [
    'wish there was', 'wish I had', 'need an app', 'looking for',
    'missing feature', 'frustrated', 'annoying', 'too complicated',
    'hate using', "can't find", "doesn't exist", 'too slow',
    'wish app', 'need tool', 'anyone know', 'too expensive',
    'waste of time', 'broken', "doesn't work", 'manual work',
    'repetitive', 'automate', 'offline', 'no ads', 'dark mode'
]

# éœ€æ±‚ç±»åˆ«å…³é”®è¯
NEED_CATEGORIES = {
    'productivity': ['task', 'todo', 'habit', 'schedule', 'calendar', 'reminder', 'focus', 'time', 'project', 'organize'],
    'finance': ['budget', 'expense', 'track', 'invest', 'portfolio', 'save', 'money', 'tax', 'bill', 'split'],
    'health': ['workout', 'exercise', 'sleep', 'diet', 'weight', 'run', 'pace', 'calorie', 'water', 'fitness'],
    'travel': ['flight', 'hotel', 'trip', 'itinerary', 'currency', 'translate', 'booking', 'map', 'packing'],
    'learning': ['note', 'learn', 'study', 'flashcard', 'book', 'course', 'knowledge', 'journal'],
    'utilities': ['simple', 'clean', 'minimal', 'fast', 'offline', 'widget', 'shortcut', 'quick']
}

def load_data():
    """åŠ è½½æ•°æ®"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    c.execute("SELECT * FROM posts")
    posts = [dict(row) for row in c.fetchall()]
    conn.close()
    return posts

def detect_pain_points(text):
    """æ£€æµ‹ç—›ç‚¹"""
    text_lower = text.lower()
    return [kw for kw in PAIN_KEYWORDS if kw in text_lower]

def categorize_needs(text):
    """åˆ†ç±»éœ€æ±‚"""
    text_lower = text.lower()
    found = []
    for category, keywords in NEED_CATEGORIES.items():
        if any(kw in text_lower for kw in keywords):
            found.append(category)
    return found if found else ['other']

def analyze(posts):
    """åˆ†æå¸–å­"""
    pain_posts = []
    keyword_counter = Counter()
    category_counts = defaultdict(list)
    
    for post in posts:
        text = f"{post['title']} {post.get('selftext', '')}"
        
        # ç—›ç‚¹æ£€æµ‹
        pains = detect_pains = detect_pain_points(text)
        if detect_pain_points:
            pain_posts.append({
                'id': post['id'],
                'subreddit': post['subreddit'],
                'title': post['title'],
                'score': post['score'],
                'pains': pains,
                'categories': categorize_needs(text)
            })
        
        # ç±»åˆ«ç»Ÿè®¡
        for cat in categorize_needs(text):
            category_counts[cat].append(post)
        
        # å…³é”®è¯æå–
        words = re.findall(r'\b[a-z]{4,}\b', text.lower())
        stop_words = {'this', 'that', 'with', 'have', 'from', 'they', 'would', 'there', 'what', 'when', 'make', 'just', 'over', 'some', 'could'}
        words = [w for w in words if w not in stop_words]
        keyword_counter.update(words)
    
    return {
        'total': len(posts),
        'pain_posts': pain_posts,
        'pain_count': len(pain_posts),
        'keywords': keyword_counter.most_common(30),
        'categories': dict(category_counts)
    }

def generate_report(analysis):
    """ç”ŸæˆæŠ¥å‘Š"""
    report = []
    report.append("# Redditéœ€æ±‚åˆ†ææŠ¥å‘Š\n")
    report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
    report.append(f"**æ•°æ®ç»Ÿè®¡**: {analysis['total']} æ¡å¸–å­ï¼Œ{analysis['pain_count']} æ¡ç—›ç‚¹å¸–å­\n\n")
    
    report.append("## ğŸ”¥ Top 20 å…³é”®è¯\n\n")
    for i, (word, count) in enumerate(analysis['keywords'][:20], 1):
        report.append(f"{i}. **{word}** - {count} æ¬¡\n")
    
    report.append("\n## ğŸ“Š éœ€æ±‚ç±»åˆ«åˆ†å¸ƒ\n\n")
    sorted_cats = sorted(analysis['categories'].items(), key=lambda x: len(x[1]), reverse=True)
    for cat, posts in sorted_cats:
        report.append(f"- **{cat.title()}**: {len(posts)} æ¡\n")
    
    report.append("\n## ğŸ’¡ Top 15 ç—›ç‚¹éœ€æ±‚\n\n")
    sorted_pain = sorted(analysis['pain_posts'], key=lambda x: x['score'], reverse=True)[:15]
    for post in sorted_pain:
        report.append(f"### [{post['title']}](https://reddit.com/{post['id']})\n")
        report.append(f"- Sub: r/{post['subreddit']} | Score: {post['score']}\n")
        report.append(f"- ç—›ç‚¹: {', '.join(post['pains'][:3])}\n")
        report.append(f"- ç±»åˆ«: {', '.join(post['categories'])}\n\n")
    
    return ''.join(report)

def main():
    print("=" * 60)
    print("Redditéœ€æ±‚åˆ†æå™¨ v2")
    print("=" * 60)
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    posts = load_data()
    print(f"   å¸–å­æ•°: {len(posts)}")
    
    print("\nğŸ” åˆ†æä¸­...")
    analysis = analyze(posts)
    
    print("\nğŸ“ ç”ŸæˆæŠ¥å‘Š...")
    report = generate_report(analysis)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = OUTPUT_DIR / 'needs_analysis_report.md'
    with open(report_path, 'w') as f:
        f.write(report)
    
    # ä¿å­˜JSON
    json_path = OUTPUT_DIR / 'analysis_results.json'
    with open(json_path, 'w') as f:
        json.dump({
            'total_posts': analysis['total'],
            'pain_point_posts': analysis['pain_count'],
            'top_keywords': analysis['keywords'][:20],
            'category_counts': {k: len(v) for k, v in analysis['categories'].items()}
        }, f, indent=2)
    
    print(f"\nâœ… å®Œæˆ!")
    print(f"ğŸ“ æŠ¥å‘Š: {report_path}")
    print(f"ğŸ“ æ•°æ®: {json_path}")

if __name__ == '__main__':
    main()
